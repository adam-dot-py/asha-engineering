{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append custom function path and bring in lookup\n",
    "import sys\n",
    "sys.path.append('/home/asha/airflow/dags/custom_functions')\n",
    "from lookup_support_provider import lookup_support_provider\n",
    "\n",
    "# packages\n",
    "import duckdb\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema = [\n",
    "    'Tenant_SK',\n",
    "    'PropertyAddress',\n",
    "    'Room', \n",
    "    'FirstName', \n",
    "    'MiddleName', \n",
    "    'LastName',\n",
    "    'DateOfBirth', \n",
    "    'NINumber', \n",
    "    'CheckinDate', \n",
    "    'CheckoutDate',\n",
    "    'NewHBClaim',\n",
    "    'HBClaimRefNumber', \n",
    "    'ReferralAgency',\n",
    "    'GroupedReferralAgency',\n",
    "    'Age', \n",
    "    'Gender', \n",
    "    'Religion',\n",
    "    'Ethnicity', \n",
    "    'Nationality', \n",
    "    'Disability', \n",
    "    'SexualOrientation',\n",
    "    'SpokenLanguage', \n",
    "    'RiskAssessment', \n",
    "    'LengthOfStay', \n",
    "    'CycleNumber',\n",
    "    'CycleNumberValue',\n",
    "    'Source',\n",
    "    'ExtractedProviderName',\n",
    "    'ProviderName',\n",
    "    'LoadDate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get cycle number from the file source\n",
    "\n",
    "# Regular expression pattern to match \"C\" followed by one or more digits\n",
    "pattern = r'C(\\d+)'\n",
    "\n",
    "# establish the load date\n",
    "load_date = datetime.now()\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "def replace_text(text):\n",
    "    \"\"\"Escapes single quotes and handles other special characters.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.replace(\"'\", \"''\")  # Double single quotes for MySQL\n",
    "    return text\n",
    "\n",
    "# motherduck config\n",
    "server_config = \"/home/asha/airflow/duckdb-config.json\"\n",
    "\n",
    "with open(server_config, \"r\") as fp:\n",
    "    config = json.load(fp)\n",
    "token = config['token']\n",
    "\n",
    "def log_execution(func):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @wraps(func)\n",
    "    def etl_task_time(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting '{func.__name__}'...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"Finished '{func.__name__}' in {time.time() - start_time} seconds.\")\n",
    "        return result\n",
    "\n",
    "    return etl_task_time\n",
    "\n",
    "def motherduck_connection(token):\n",
    "    def connection_decorator(func):\n",
    "        con = duckdb.connect(f'md:?motherduck_token={token}')\n",
    "        \n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # pass con as a keyword argument for use in other functions\n",
    "            return func(*args, con=con, **kwargs)\n",
    "    \n",
    "        return wrapper\n",
    "    return connection_decorator\n",
    "\n",
    "@log_execution\n",
    "@motherduck_connection(token=token)\n",
    "def job_load_tenant_data(bronze_schema, bronze_table_name, con, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to motherduck\n",
    "    con.sql(\"USE asha_production;\")\n",
    "    con.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema};\")\n",
    "    \n",
    "    # iterate over files\n",
    "    all_files = []\n",
    "    p = Path(os.getcwd())\n",
    "    source_path = p / 'source'\n",
    "    processing_path = p / 'processing'\n",
    "    processed_path = p / 'processed'\n",
    "    error_path = p / 'errors'\n",
    "    \n",
    "    load_date = datetime.now()\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for dir in dirs:\n",
    "                dir_path = os.path.join(root, dir)\n",
    "                for file in os.listdir(dir_path):\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    if os.path.isfile(file_path) and file.endswith('.xlsx'):\n",
    "                        source_dir = os.path.join(source_path, dir, file)\n",
    "                        processing_dir = os.path.join(processing_path, dir)\n",
    "                        processed_dir = os.path.join(processed_path, dir)\n",
    "                        error_dir = os.path.join(error_path, dir)\n",
    "                        \n",
    "                        os.makedirs(processing_dir, exist_ok=True)\n",
    "                        os.makedirs(processed_dir, exist_ok=True)\n",
    "                        os.makedirs(error_dir, exist_ok=True)\n",
    "                        \n",
    "                        # process the file\n",
    "                        processing_file_path = os.path.join(processing_dir, file)\n",
    "                        shutil.move(source_dir, processing_file_path)\n",
    "                        excel_file = pd.ExcelFile(processing_file_path)\n",
    "                        \n",
    "                        # only grab the sheet where the expected columns are present\n",
    "                        selected_sheet = None\n",
    "                        for sheet in excel_file.sheet_names:\n",
    "                            first_row = pd.read_excel(excel_file, sheet, nrows=1)\n",
    "                            column_headers = first_row.columns.tolist()\n",
    "                            \n",
    "                            key_columns = [\"PropertyAddress\", \"FirstName\", \"LastName\", \"Ethnicity\", \"Nationality\"]\n",
    "\n",
    "                            if set(key_columns) <= set(column_headers):  # Check if all key columns are present in headers\n",
    "                                selected_sheet = sheet\n",
    "                                break\n",
    "                            \n",
    "                        # If a sheet containing the BC column headers is found, load it into a DataFrame\n",
    "                        try:\n",
    "                            if selected_sheet is not None:\n",
    "                                df = pd.read_excel(excel_file, selected_sheet)\n",
    "                                df = df.dropna(subset=key_columns)\n",
    "                                \n",
    "                                # trim the dataframe if an object is a string, removing whitespace\n",
    "                                df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "                                \n",
    "                                # this gets us the cycle number and value\n",
    "                                df['CycleNumber'] = dir\n",
    "                                cycle_pattern = r'[^\\d]+(\\d+)[^\\d]+'\n",
    "                                cycle_number = re.sub(cycle_pattern, r'\\1', dir)\n",
    "                                df['CycleNumberValue'] = cycle_number\n",
    "                                \n",
    "                                # this sections gets us the source file for the record, as well as the provider\n",
    "                                source_pattern = r'(?i)C\\d+ TL'\n",
    "                                df['Source'] = file.replace('.xlsx','')\n",
    "                                df['ExtractedProviderName'] = df['Source'].str.replace(source_pattern, '', regex=True)\n",
    "                                df['ProviderName'] = df['ExtractedProviderName'].apply(lambda x: lookup_support_provider(x))\n",
    "                        \n",
    "                                # add load date\n",
    "                                df['LoadDate'] = load_date\n",
    "                            \n",
    "                                # adds group referral agency for later - this is cleaned by a seperate process in airflow\n",
    "                                df['GroupedReferralAgency'] = df['ReferralAgency']\n",
    "                                \n",
    "                                # this section creates a UID for each tenant, by concating strings and then shortening to length\n",
    "                                df['Tenant_SK'] = df['LastName'].replace(' ','').fillna(df['FirstName']).astype(str).str.cat(df['NINumber'].replace(' ','').fillna(df['HBClaimRefNumber']).replace(' ','').astype(str), sep='_') # worst code ever\n",
    "                                df['Tenant_SK'] = df['Tenant_SK'].str.upper()\n",
    "                                df['Tenant_SK'] = df['Tenant_SK'].str.slice(0,100) # shorten\n",
    "                                \n",
    "                                # drop any duplicate SKs\n",
    "                                df = df.drop_duplicates(subset=['Tenant_SK'], keep='last')\n",
    "\n",
    "                                all_files.append(df)\n",
    "                                processed_file_path = os.path.join(processed_dir, file)\n",
    "                                shutil.move(processing_file_path, processed_file_path)\n",
    "                                print(f\"Processed -> {file}\")\n",
    "                                \n",
    "                                # check if any records are incorrect - if so, move to errors\n",
    "                                # error_check = df.loc[df['RecordStatus'] != 'Correct']\n",
    "                                # error_check = (df['RecordStatus'] != 'Correct').any()\n",
    "                                # if error_check:\n",
    "                                #     shutil.move(processing_dir, error_dir)\n",
    "                                #     logging.error(f\"{file} -> failed due to not passing record checks\")\n",
    "                                # else:\n",
    "                                #     # if no errors, move to processed\n",
    "                                #     processed_data.append(df)\n",
    "                                #     shutil.move(processing_dir, processed_dir)     \n",
    "                                \n",
    "                            else: # if no sheet found, move to errors\n",
    "                                error_file_path = os.path.join(error_dir, file)\n",
    "                                shutil.move(processing_file_path, error_file_path)\n",
    "                                logging.error(f\"{file} -> failed due to mismatched column headers\\n\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Found error with {file} -> {e}\")\n",
    "                            pass\n",
    "        \n",
    "    # concat everything together\n",
    "    fdf = pd.concat(all_files)\n",
    "    \n",
    "    # if the column does not exist, create it as a None value\n",
    "    try:\n",
    "        fdf = fdf[database_schema]\n",
    "    except:\n",
    "        for col in database_schema:\n",
    "            if col not in fdf.columns:\n",
    "                fdf[col] = None\n",
    "        fdf = fdf[database_schema]\n",
    "    fdf = fdf.replace(to_replace={np.nan : \"UNKNOWN\", '' : \"UNKNOWN\", ' ' : \"UNKNOWN\", \"nan\" : \"UNKNOWN\"})\n",
    "    for col in fdf.columns:\n",
    "        fdf[col] = fdf[col].apply(lambda x: replace_text(str(x)))\n",
    "    \n",
    "    # minor transformation\n",
    "    column_data_types = {\n",
    "        'Tenant_SK' : 'VARCHAR (100)',\n",
    "        'PropertyAddress' : 'VARCHAR (100)',\n",
    "        'Room' : 'VARCHAR(30)', \n",
    "        'FirstName' : 'VARCHAR(100)', \n",
    "        'MiddleName': 'VARCHAR(100)', \n",
    "        'LastName': 'VARCHAR(100)',\n",
    "        'DateOfBirth' : 'VARCHAR(30)', \n",
    "        'NINumber' : 'VARCHAR(30)', \n",
    "        'CheckinDate' : 'VARCHAR(30)', \n",
    "        'CheckoutDate': 'VARCHAR(30)' ,\n",
    "        'NewHBClaim': 'VARCHAR(30)',\n",
    "        'HBClaimRefNumber': 'VARCHAR(100)', \n",
    "        'ReferralAgency': 'VARCHAR(100)',\n",
    "        'GroupedReferralAgency' : 'VARCHAR(100)',\n",
    "        'Age' : \"VARCHAR(10)\", \n",
    "        'Gender' : 'VARCHAR(30)', \n",
    "        'Religion' : 'VARCHAR(200)',\n",
    "        'Ethnicity' : 'VARCHAR(200)', \n",
    "        'Nationality' : 'VARCHAR(200)', \n",
    "        'Disability' : 'VARCHAR(200)', \n",
    "        'SexualOrientation' : 'VARCHAR(100)',\n",
    "        'SpokenLanguage': 'VARCHAR(100)', \n",
    "        'RiskAssessment': 'VARCHAR(100)', \n",
    "        'LengthOfStay': 'VARCHAR(100)', \n",
    "        'CycleNumber' : 'VARCHAR(100)',\n",
    "        'CycleNumberValue' : 'INT',\n",
    "        'Source' : 'VARCHAR(100)',\n",
    "        'ExtractedProviderName': 'VARCHAR(100)',\n",
    "        'ProviderName' : 'VARCHAR(100)',\n",
    "        'LoadDate': 'DATETIME'\n",
    "    }\n",
    "    \n",
    "    # need code here to check if data has changed\n",
    "    \n",
    "    # write to motherduck\n",
    "    con.sql(f\"INSERT INTO {bronze_schema}.{bronze_table_name} SELECT * FROM fdf;\")\n",
    "    con.close()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # this is the ETL task\n",
    "    bronze_schema = 'bronze'\n",
    "    bronze_table_name = 'tenant_data'\n",
    "    \n",
    "    job_load_tenant_data(\n",
    "        token=token,\n",
    "        bronze_schema=bronze_schema,\n",
    "        bronze_table_name=bronze_table_name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
