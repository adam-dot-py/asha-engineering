{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the basic logging level, for our purposes we will use INFO\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings about the data validations\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 'job_load_yearly_tenant_data'...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 146\u001b[0m\n\u001b[1;32m    143\u001b[0m bronze_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbronze\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    144\u001b[0m bronze_table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myearly_tenant_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 146\u001b[0m \u001b[43mjob_load_yearly_tenant_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbronze_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbronze_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbronze_table_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbronze_table_name\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mlog_execution.<locals>.etl_task_time\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m, in \u001b[0;36mmotherduck_connection.<locals>.connection_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# pass con as a keyword argument for use in other functions\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m, in \u001b[0;36mjob_load_yearly_tenant_data\u001b[0;34m(bronze_schema, bronze_table_name, con, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 shutil\u001b[38;5;241m.\u001b[39mmove(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessing_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# concat everything together\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m fdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# minor transformation\u001b[39;00m\n\u001b[1;32m     94\u001b[0m fdf\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirstName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLastName\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/airflow_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/airflow_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/airflow_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# establish the load date\n",
    "load_date = datetime.now()\n",
    "\n",
    "# this function allows for ' with the mysql insert statement - essential\n",
    "def replace_text(text):\n",
    "  \"\"\"Escapes single quotes within a string for safe MySQL insertion.\"\"\"\n",
    "  return text.replace(\"'\", \"\\\\'\")\n",
    "\n",
    "# motherduck config\n",
    "server_config = \"/home/asha/airflow/duckdb-config.json\"\n",
    "\n",
    "with open(server_config, \"r\") as fp:\n",
    "    config = json.load(fp)\n",
    "token = config['token']\n",
    "\n",
    "def log_execution(func):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @wraps(func)\n",
    "    def etl_task_time(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting '{func.__name__}'...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"Finished '{func.__name__}' in {time.time() - start_time} seconds.\")\n",
    "        return result\n",
    "\n",
    "    return etl_task_time\n",
    "\n",
    "def motherduck_connection(token):\n",
    "    def connection_decorator(func):\n",
    "        con = duckdb.connect(f'md:?motherduck_token={token}')\n",
    "        \n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # pass con as a keyword argument for use in other functions\n",
    "            return func(*args, con=con, **kwargs)\n",
    "    \n",
    "        return wrapper\n",
    "    return connection_decorator\n",
    "\n",
    "@log_execution\n",
    "@motherduck_connection(token=token)\n",
    "def job_load_yearly_tenant_data(bronze_schema, bronze_table_name, con, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # connect to motherduck\n",
    "    con.sql(\"USE asha_production;\")\n",
    "    con.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema};\")\n",
    "    \n",
    "    # iterate over files\n",
    "    all_files = []\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    source_path = f'{current_dir}/source'\n",
    "    processing_path = f'{current_dir}/processing'\n",
    "    processed_path = f'{current_dir}/processed'\n",
    "    error_path = f'{current_dir}/errors'\n",
    "\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            shutil.move(file_path, f'{processing_path}/{file}')\n",
    "            \n",
    "            if file.endswith('.xlsx'):\n",
    "                try:\n",
    "                    df = pd.read_excel(f'{processing_path}/{file}', sheet_name='template')\n",
    "                    df['Source'] = file\n",
    "                    df['LoadDate'] = load_date\n",
    "                    all_files.append(df)\n",
    "                    shutil.move(f'{processing_path}/{file}', f'{processed_path}/{file}')\n",
    "                    logging.info(f\"Finished processing -> {file}\")\n",
    "                except:\n",
    "                    logging.warning(f\"Error processing file -> {file}\")\n",
    "                    shutil.move(f'{processing_path}/{file}', f'{error_path}/{file}')\n",
    "            \n",
    "            if file.endswith('.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(f'{processing_path}/{file}')\n",
    "                    df['Source'] = file\n",
    "                    df['LoadDate'] = load_date\n",
    "                    all_files.append(df)\n",
    "                    shutil.move(f'{processing_path}/{file}', f'{processed_path}/{file}')\n",
    "                    logging.info(f\"Finished processing -> {file}\")\n",
    "                except:\n",
    "                    logging.warning(f\"Error processing file -> {file}\")\n",
    "                    shutil.move(f'{processing_path}/{file}', f'{error_path}/{file}')\n",
    "    \n",
    "    # concat everything together\n",
    "    fdf = pd.concat(all_files)\n",
    "    \n",
    "    # minor transformation\n",
    "    fdf.dropna(subset=['FirstName', 'LastName'], inplace=True)\n",
    "    fdf['Disability'] = fdf['Disability'].str.replace(',', ' |')\n",
    "    fdf = fdf.replace(to_replace={np.nan : \"UNKNOWN\", '' : \"UNKNOWN\", ' ' : \"UNKNOWN\", \"nan\" : \"UNKNOWN\"})\n",
    "    for col in fdf.columns:\n",
    "        fdf[col] = fdf[col].apply(lambda x: replace_text(str(x)))\n",
    "    \n",
    "    column_data_types = {\n",
    "        'PropertyAddress' : 'VARCHAR (100)',\n",
    "        'Room' : 'VARCHAR(30)', \n",
    "        'FirstName' : 'VARCHAR(100)', \n",
    "        'MiddleName': 'VARCHAR(100)', \n",
    "        'LastName': 'VARCHAR(100)',\n",
    "        'DateOfBirth' : 'VARCHAR(30)', \n",
    "        'NINumber' : 'VARCHAR(30)', \n",
    "        'CheckinDate' : 'VARCHAR(30)', \n",
    "        'CheckoutDate': 'VARCHAR(30)' ,\n",
    "        'NewHBClaim': 'VARCHAR(30)',\n",
    "        'HBClaimRefNumber': 'VARCHAR(30)', \n",
    "        'ReferralAgency': 'VARCHAR(100)', \n",
    "        'Age' : \"VARCHAR(10)\", \n",
    "        'Gender' : 'VARCHAR(30)', \n",
    "        'Religion' : 'VARCHAR(200)',\n",
    "        'Ethnicity' : 'VARCHAR(200)', \n",
    "        'Nationality' : 'VARCHAR(200)', \n",
    "        'Disability' : 'VARCHAR(200)', \n",
    "        'SexualOrientation' : 'VARCHAR(100)',\n",
    "        'SpokenLanguage': 'VARCHAR(100)', \n",
    "        'RiskAssessment': 'VARCHAR(100)', \n",
    "        'LengthOfStay': 'VARCHAR(100)', \n",
    "        'CycleNumber' : 'VARCHAR(10)',\n",
    "        'RecordStatus' : 'VARCHAR(100)',\n",
    "        'Source' : 'VARCHAR(100)',\n",
    "        'LoadDate': 'DATETIME'\n",
    "    }\n",
    "    \n",
    "    # create the column headers and convert to a string\n",
    "    column_headers = [f\"{col} {data_type}\" for col, data_type in column_data_types.items()]\n",
    "    column_headers_string = \", \".join(column_headers)\n",
    "    \n",
    "    # create the table if it does not exist\n",
    "    try:\n",
    "       con.sql(f\"CREATE OR REPLACE TABLE {bronze_schema}.{bronze_table_name} AS SELECT * FROM fdf;\")\n",
    "    finally:\n",
    "        # commit changes and close connections\n",
    "        con.close()\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # this is the ETL task\n",
    "    bronze_schema = 'bronze'\n",
    "    bronze_table_name = 'yearly_tenant_data'\n",
    "\n",
    "    job_load_yearly_tenant_data(\n",
    "        token=token,\n",
    "        bronze_schema=bronze_schema,\n",
    "        bronze_table_name=bronze_table_name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
